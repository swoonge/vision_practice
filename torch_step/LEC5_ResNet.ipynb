{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN신경망"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.init as init\n",
    "\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 공식 코드\n",
    "def conv3x3(in_planes: int, out_planes: int, stride: int = 1, groups: int = 1, dilation: int = 1) -> nn.Conv2d:\n",
    "    \"\"\"3x3 convolution with padding\"\"\"\n",
    "    return nn.Conv2d(\n",
    "        in_planes,\n",
    "        out_planes,\n",
    "        kernel_size=3,\n",
    "        stride=stride,\n",
    "        padding=dilation,\n",
    "        groups=groups,\n",
    "        bias=False,\n",
    "        dilation=dilation,\n",
    "    ) # nn.Conv2 함수에 대한 인자를 모두 규정해 주는 듯\n",
    "\n",
    "def conv1x1(in_planes: int, out_planes: int, stride: int = 1) -> nn.Conv2d:\n",
    "    \"\"\"1x1 convolution\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Optional' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mBasicBlock\u001b[39;00m(nn\u001b[39m.\u001b[39mModule):\n\u001b[1;32m      2\u001b[0m     expansion: \u001b[39mint\u001b[39m \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m      4\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[1;32m      5\u001b[0m         \u001b[39mself\u001b[39m,\n\u001b[1;32m      6\u001b[0m         inplanes: \u001b[39mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m         norm_layer: Optional[Callable[\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m, nn\u001b[39m.\u001b[39mModule]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m     14\u001b[0m     ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n",
      "Cell \u001b[0;32mIn[9], line 9\u001b[0m, in \u001b[0;36mBasicBlock\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mBasicBlock\u001b[39;00m(nn\u001b[39m.\u001b[39mModule):\n\u001b[1;32m      2\u001b[0m     expansion: \u001b[39mint\u001b[39m \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m      4\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[1;32m      5\u001b[0m         \u001b[39mself\u001b[39m,\n\u001b[1;32m      6\u001b[0m         inplanes: \u001b[39mint\u001b[39m,\n\u001b[1;32m      7\u001b[0m         planes: \u001b[39mint\u001b[39m,\n\u001b[1;32m      8\u001b[0m         stride: \u001b[39mint\u001b[39m \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m,\n\u001b[0;32m----> 9\u001b[0m         downsample: Optional[nn\u001b[39m.\u001b[39mModule] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m     10\u001b[0m         groups: \u001b[39mint\u001b[39m \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m,\n\u001b[1;32m     11\u001b[0m         base_width: \u001b[39mint\u001b[39m \u001b[39m=\u001b[39m \u001b[39m64\u001b[39m,\n\u001b[1;32m     12\u001b[0m         dilation: \u001b[39mint\u001b[39m \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m,\n\u001b[1;32m     13\u001b[0m         norm_layer: Optional[Callable[\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m, nn\u001b[39m.\u001b[39mModule]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m     14\u001b[0m     ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     15\u001b[0m         \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m()\n\u001b[1;32m     16\u001b[0m         \u001b[39mif\u001b[39;00m norm_layer \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Optional' is not defined"
     ]
    }
   ],
   "source": [
    "# 기본 블럭. resnet32 까지 사용\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion: int = 1\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        inplanes: int,\n",
    "        planes: int,\n",
    "        stride: int = 1,\n",
    "        downsample: Optional[nn.Module] = None,\n",
    "        groups: int = 1,\n",
    "        base_width: int = 64,\n",
    "        dilation: int = 1,\n",
    "        norm_layer: Optional[Callable[..., nn.Module]] = None,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "        if groups != 1 or base_width != 64:\n",
    "            raise ValueError(\"BasicBlock only supports groups=1 and base_width=64\")\n",
    "        if dilation > 1:\n",
    "            raise NotImplementedError(\"Dilation > 1 not supported in BasicBlock\")\n",
    "        # Both self.conv1 and self.downsample layers downsample the input when stride != 1\n",
    "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
    "        self.bn1 = norm_layer(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "        self.bn2 = norm_layer(planes)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resnet50 부터는 보틀넥 구조 사용\n",
    "class Bottleneck(nn.Module):\n",
    "    # Bottleneck in torchvision places the stride for downsampling at 3x3 convolution(self.conv2)\n",
    "    # while original implementation places the stride at the first 1x1 convolution(self.conv1)\n",
    "    # according to \"Deep residual learning for image recognition\" https://arxiv.org/abs/1512.03385.\n",
    "    # This variant is also known as ResNet V1.5 and improves accuracy according to\n",
    "    # https://ngc.nvidia.com/catalog/model-scripts/nvidia:resnet_50_v1_5_for_pytorch.\n",
    "\n",
    "    expansion: int = 4\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        inplanes: int,\n",
    "        planes: int,\n",
    "        stride: int = 1,\n",
    "        downsample: Optional[nn.Module] = None,\n",
    "        groups: int = 1,\n",
    "        base_width: int = 64,\n",
    "        dilation: int = 1,\n",
    "        norm_layer: Optional[Callable[..., nn.Module]] = None,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "        width = int(planes * (base_width / 64.0)) * groups\n",
    "        # Both self.conv2 and self.downsample layers downsample the input when stride != 1\n",
    "        self.conv1 = conv1x1(inplanes, width)\n",
    "        self.bn1 = norm_layer(width)\n",
    "        self.conv2 = conv3x3(width, width, stride, groups, dilation)\n",
    "        self.bn2 = norm_layer(width)\n",
    "        self.conv3 = conv1x1(width, planes * self.expansion)\n",
    "        self.bn3 = norm_layer(planes * self.expansion)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        block: Type[Union[BasicBlock, Bottleneck]],\n",
    "        layers: List[int],\n",
    "        num_classes: int = 1000,\n",
    "        zero_init_residual: bool = False,\n",
    "        groups: int = 1,\n",
    "        width_per_group: int = 64,\n",
    "        replace_stride_with_dilation: Optional[List[bool]] = None,\n",
    "        norm_layer: Optional[Callable[..., nn.Module]] = None,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        _log_api_usage_once(self)\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "        self._norm_layer = norm_layer\n",
    "\n",
    "        self.inplanes = 64\n",
    "        self.dilation = 1\n",
    "        if replace_stride_with_dilation is None:\n",
    "            # each element in the tuple indicates if we should replace\n",
    "            # the 2x2 stride with a dilated convolution instead\n",
    "            replace_stride_with_dilation = [False, False, False]\n",
    "        if len(replace_stride_with_dilation) != 3:\n",
    "            raise ValueError(\n",
    "                \"replace_stride_with_dilation should be None \"\n",
    "                f\"or a 3-element tuple, got {replace_stride_with_dilation}\"\n",
    "            )\n",
    "        self.groups = groups\n",
    "        self.base_width = width_per_group\n",
    "        self.conv1 = nn.Conv2d(3, self.inplanes, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.bn1 = norm_layer(self.inplanes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2, dilate=replace_stride_with_dilation[0])\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2, dilate=replace_stride_with_dilation[1])\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2, dilate=replace_stride_with_dilation[2])\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
    "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "        # Zero-initialize the last BN in each residual branch,\n",
    "        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n",
    "        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\n",
    "        if zero_init_residual:\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, Bottleneck) and m.bn3.weight is not None:\n",
    "                    nn.init.constant_(m.bn3.weight, 0)  # type: ignore[arg-type]\n",
    "                elif isinstance(m, BasicBlock) and m.bn2.weight is not None:\n",
    "                    nn.init.constant_(m.bn2.weight, 0)  # type: ignore[arg-type]\n",
    "\n",
    "    def _make_layer(\n",
    "        self,\n",
    "        block: Type[Union[BasicBlock, Bottleneck]],\n",
    "        planes: int,\n",
    "        blocks: int,\n",
    "        stride: int = 1,\n",
    "        dilate: bool = False,\n",
    "    ) -> nn.Sequential:\n",
    "        norm_layer = self._norm_layer\n",
    "        downsample = None\n",
    "        previous_dilation = self.dilation\n",
    "        if dilate:\n",
    "            self.dilation *= stride\n",
    "            stride = 1\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                conv1x1(self.inplanes, planes * block.expansion, stride),\n",
    "                norm_layer(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(\n",
    "            block(\n",
    "                self.inplanes, planes, stride, downsample, self.groups, self.base_width, previous_dilation, norm_layer\n",
    "            )\n",
    "        )\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(\n",
    "                block(\n",
    "                    self.inplanes,\n",
    "                    planes,\n",
    "                    groups=self.groups,\n",
    "                    base_width=self.base_width,\n",
    "                    dilation=self.dilation,\n",
    "                    norm_layer=norm_layer,\n",
    "                )\n",
    "            )\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def _forward_impl(self, x: Tensor) -> Tensor:\n",
    "        # See note [TorchScript super()]\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        return self._forward_impl(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _resnet(\n",
    "    block: Type[Union[BasicBlock, Bottleneck]],\n",
    "    layers: List[int],\n",
    "    weights: Optional[WeightsEnum],\n",
    "    progress: bool,\n",
    "    **kwargs: Any,\n",
    ") -> ResNet: # 리턴 타입이 ResNet 객체라는 것을 명시\n",
    "    if weights is not None:\n",
    "        _ovewrite_named_param(kwargs, \"num_classes\", len(weights.meta[\"categories\"]))\n",
    "\n",
    "    model = ResNet(block, layers, **kwargs)\n",
    "\n",
    "    if weights is not None:\n",
    "        model.load_state_dict(weights.get_state_dict(progress=progress))\n",
    "\n",
    "    return model\n",
    "    # 결국 이 아래의 함수들은 모듈화를 위한 함수들인 것. 최종 리턴은 ResNet 객체\n",
    "\n",
    "_COMMON_META = {\n",
    "    \"min_size\": (1, 1),\n",
    "    \"categories\": _IMAGENET_CATEGORIES,\n",
    "}\n",
    "\n",
    "class ResNet50_Weights(WeightsEnum):\n",
    "    IMAGENET1K_V1 = Weights(\n",
    "        url=\"https://download.pytorch.org/models/resnet50-0676ba61.pth\",\n",
    "        transforms=partial(ImageClassification, crop_size=224),\n",
    "        meta={\n",
    "            **_COMMON_META,\n",
    "            \"num_params\": 25557032,\n",
    "            \"recipe\": \"https://github.com/pytorch/vision/tree/main/references/classification#resnet\",\n",
    "            \"_metrics\": {\n",
    "                \"ImageNet-1K\": {\n",
    "                    \"acc@1\": 76.130,\n",
    "                    \"acc@5\": 92.862,\n",
    "                }\n",
    "            },\n",
    "            \"_ops\": 4.089,\n",
    "            \"_file_size\": 97.781,\n",
    "            \"_docs\": \"\"\"These weights reproduce closely the results of the paper using a simple training recipe.\"\"\",\n",
    "        },\n",
    "    )\n",
    "    IMAGENET1K_V2 = Weights(\n",
    "        url=\"https://download.pytorch.org/models/resnet50-11ad3fa6.pth\",\n",
    "        transforms=partial(ImageClassification, crop_size=224, resize_size=232),\n",
    "        meta={\n",
    "            **_COMMON_META,\n",
    "            \"num_params\": 25557032,\n",
    "            \"recipe\": \"https://github.com/pytorch/vision/issues/3995#issuecomment-1013906621\",\n",
    "            \"_metrics\": {\n",
    "                \"ImageNet-1K\": {\n",
    "                    \"acc@1\": 80.858,\n",
    "                    \"acc@5\": 95.434,\n",
    "                }\n",
    "            },\n",
    "            \"_ops\": 4.089,\n",
    "            \"_file_size\": 97.79,\n",
    "            \"_docs\": \"\"\"\n",
    "                These weights improve upon the results of the original paper by using TorchVision's `new training recipe\n",
    "                <https://pytorch.org/blog/how-to-train-state-of-the-art-models-using-torchvision-latest-primitives/>`_.\n",
    "            \"\"\",\n",
    "        },\n",
    "    )\n",
    "    DEFAULT = IMAGENET1K_V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resnet50 을 불러온다고 가정했을 때, \"return _resnet(Bottleneck, [3, 4, 6, 3], weights, progress, **kwargs)\"을 리턴한다.\n",
    "@register_model()\n",
    "@handle_legacy_interface(weights=(\"pretrained\", ResNet50_Weights.IMAGENET1K_V1))\n",
    "def resnet50(*, weights: Optional[ResNet50_Weights] = None, progress: bool = True, **kwargs: Any) -> ResNet:\n",
    "    \"\"\"ResNet-50 from `Deep Residual Learning for Image Recognition <https://arxiv.org/pdf/1512.03385.pdf>`__.\n",
    "\n",
    "    .. note::\n",
    "       The bottleneck of TorchVision places the stride for downsampling to the second 3x3\n",
    "       convolution while the original paper places it to the first 1x1 convolution.\n",
    "       This variant improves the accuracy and is known as `ResNet V1.5\n",
    "       <https://ngc.nvidia.com/catalog/model-scripts/nvidia:resnet_50_v1_5_for_pytorch>`_.\n",
    "\n",
    "    Args:\n",
    "        weights (:class:`~torchvision.models.ResNet50_Weights`, optional): The\n",
    "            pretrained weights to use. See\n",
    "            :class:`~torchvision.models.ResNet50_Weights` below for\n",
    "            more details, and possible values. By default, no pre-trained\n",
    "            weights are used.\n",
    "        progress (bool, optional): If True, displays a progress bar of the\n",
    "            download to stderr. Default is True.\n",
    "        **kwargs: parameters passed to the ``torchvision.models.resnet.ResNet``\n",
    "            base class. Please refer to the `source code\n",
    "            <https://github.com/pytorch/vision/blob/main/torchvision/models/resnet.py>`_\n",
    "            for more details about this class.\n",
    "\n",
    "    .. autoclass:: torchvision.models.ResNet50_Weights\n",
    "        :members:\n",
    "    \"\"\"\n",
    "    weights = ResNet50_Weights.verify(weights)\n",
    "\n",
    "    return _resnet(Bottleneck, [3, 4, 6, 3], weights, progress, **kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256*2\n",
    "learning_rate = 0.0002\n",
    "num_epoch = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m CIFAR10_train\u001b[39m=\u001b[39mdset\u001b[39m.\u001b[39mCIFAR10(\u001b[39m\"\u001b[39m\u001b[39m./\u001b[39m\u001b[39m\"\u001b[39m, train\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, transform\u001b[39m=\u001b[39mtransforms\u001b[39m.\u001b[39mToTensor(), target_transform\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, download\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m      2\u001b[0m CIFAR10_test\u001b[39m=\u001b[39mdset\u001b[39m.\u001b[39mCIFAR10(\u001b[39m\"\u001b[39m\u001b[39m./\u001b[39m\u001b[39m\"\u001b[39m, train\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, transform\u001b[39m=\u001b[39mtransforms\u001b[39m.\u001b[39mToTensor(), target_transform\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, download\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dset' is not defined"
     ]
    }
   ],
   "source": [
    "CIFAR10_train=dset.CIFAR10(\"./\", train=True, transform=transforms.ToTensor(), target_transform=None, download=True)\n",
    "CIFAR10_test=dset.CIFAR10(\"./\", train=False, transform=transforms.ToTensor(), target_transform=None, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(CIFAR10_train, batch_size=batch_size, shuffle=True, num_workers=8, drop_last=True)\n",
    "test_loader = DataLoader(CIFAR10_test, batch_size=batch_size, shuffle=True, num_workers=8, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device=torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model=VGG16(num_classes=100, base_dim=32).to(device=device)\n",
    "loss_func=nn.CrossEntropyLoss()\n",
    "optimizer=optim.Adam(model.parameters(), lr=learning_rate)\n",
    "loss_array = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 is start\n",
      "tensor(0.0022, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 1 is start\n",
      "tensor(1.0734e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 2 is start\n",
      "tensor(5.0426e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 3 is start\n",
      "tensor(4.1279e-06, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 4 is start\n",
      "tensor(4.4722e-06, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 5 is start\n",
      "tensor(1.1446e-06, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 6 is start\n",
      "tensor(2.2774e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 7 is start\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mepoch\u001b[39m\u001b[39m\"\u001b[39m, i, \u001b[39m\"\u001b[39m\u001b[39mis start\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[39mfor\u001b[39;00m j, [img, label] \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(train_loader):\n\u001b[0;32m----> 5\u001b[0m     x \u001b[39m=\u001b[39m img\u001b[39m.\u001b[39;49mto(device)\n\u001b[1;32m      6\u001b[0m     y_ \u001b[39m=\u001b[39m label\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m      8\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "optimizer=optim.Adam(model.parameters(), lr=learning_rate/50)\n",
    "for i in range(num_epoch):\n",
    "    print(\"epoch\", i, \"is start\")\n",
    "    for j, [img, label] in enumerate(train_loader):\n",
    "        x = img.to(device)\n",
    "        y_ = label.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output= model.forward(x)\n",
    "        loss = loss_func(output, y_)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if j % 100 == 0:\n",
    "            loss_array.append(loss.cpu().detach().numpy())\n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(loss_array[:])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Test Data: 76.171875\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for img, label in test_loader:\n",
    "        x = img.to(device)\n",
    "        y_ = label.to(device)\n",
    "\n",
    "        output = model.forward(x)\n",
    "        _, output_index = torch.max(output, 1)\n",
    "\n",
    "        total += label.size(0)\n",
    "        correct += (output_index == y_).sum().float()\n",
    "\n",
    "    print(\"Accuracy of Test Data: {}\".format(100*correct/total))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.16 ('py39')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7350abcaff91871cc2fda5041ddf4038d94572c9fca8856fb0fc40a5853d68d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
